{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import logging\n",
    "import torchaudio\n",
    "import speechbrain as sb\n",
    "from tqdm.contrib import tqdm\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from speechbrain.utils.metric_stats import EER, minDCF\n",
    "from speechbrain.utils.data_utils import download_file\n",
    "from speechbrain.utils.distributed import run_on_main\n",
    "import shutil\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verification import compute_embedding,compute_embedding_loop,get_verification_scores,dataio_prep\n",
    "from voxceleb_prepare import prepare_voxceleb  # noqa E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechbrain.lobes.features import Fbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {} ## 参数列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/qinyc/02_exp/notebook/model/\")\n",
    "from ECAPA_TDNN_2 import ECAPA_TDNN\n",
    "\n",
    "params[\"output_folder\"] = \"/home/qinyc/02_exp/exp13/\"\n",
    "params['pretrain_path'] = '/home/qinyc/02_exp/exp13/save/CKPT+2022-03-19+04-49-19+00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"device\"] = \"cuda:0\"\n",
    "\n",
    "params[\"voxceleb_source\"] = \"/data0/qyc/vox1_2\"\n",
    "params[\"data_folder\"] = \"/data0/qyc/vox1_2/vox1_test\"\n",
    "\n",
    "params[\"save_folder\"] = os.path.join(params[\"output_folder\"],\"save\")\n",
    "params[\"train_data\"] = \"/home/qinyc/02_exp/exp/train.csv\"\n",
    "\n",
    "params['enrol_data'] = \"/home/qinyc/02_exp/exp/enrol_O.csv\"\n",
    "params['test_data']=\"/home/qinyc/02_exp/exp/test_O.csv\"\n",
    "\n",
    "veri_file_path = \"/home/qinyc/02_exp/exp/veri_test2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"train_dataloader_opts\"] = {\"batch_size\":1}\n",
    "params[\"enrol_dataloader_opts\"] = {\"batch_size\":1}\n",
    "params[\"test_dataloader_opts\"] = {\"batch_size\":1}\n",
    "\n",
    "params[\"compute_features\"] = Fbank(n_mels=80)\n",
    "params[\"mean_var_norm\"] = sb.processing.features.InputNormalization(norm_type=\"sentence\",std_norm=False)\n",
    "params[\"embedding_model\"] = ECAPA_TDNN(input_size=80)\n",
    "params[\"mean_var_norm_emb\"] = sb.processing.features.InputNormalization(norm_type=\"global\",std_norm=False)\n",
    "params[\"pretrainer\"] = sb.utils.parameter_transfer.Pretrainer(collect_in=params[\"save_folder\"],\\\n",
    "                                                                       loadables={\"embedding_model\": params[\"embedding_model\"]},\\\n",
    "                                                                       paths={\"embedding_model\": os.path.join(params[\"pretrain_path\"],\"embedding_model.ckpt\")})\n",
    "params[\"left_frames\"]=0\n",
    "params[\"right_frames\"]=0\n",
    "params[\"deltas\"] = False\n",
    "params[\"score_norm\"]=\"s-norm\"\n",
    "params[\"cohort_size\"]=2000\n",
    "params[\"n_train_snts\"]=40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: /home/qinyc/02_exp/exp13/\n"
     ]
    }
   ],
   "source": [
    "params_file = \"verification.yaml\"\n",
    "sb.core.create_experiment_directory(\n",
    "    experiment_directory=params[\"output_folder\"],\n",
    "    hyperparams_to_save=params_file,\n",
    "    overrides=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 4715 4713\n"
     ]
    }
   ],
   "source": [
    "## 设置数据加载的内容\n",
    "# voxceleb - O 的数量为 40000 4715 4713 * 1   40000 4715 4713\n",
    "# voxceleb - E 的数量为 40000 145160 142540 * 1 \n",
    "# voxceleb - H 的数量为 40000 137924 135415 * 1\n",
    "\n",
    "## batch-size：采用1 \n",
    "\n",
    "train_dataloader, enrol_dataloader, test_dataloader = dataio_prep(params)\n",
    "print(len(train_dataloader) ,len(enrol_dataloader) ,len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.pretrained.fetching - Fetch embedding_model.ckpt: Linking to local file in /home/qinyc/02_exp/exp13/save/CKPT+2022-03-19+04-49-19+00/embedding_model.ckpt.\n",
      "speechbrain.utils.parameter_transfer - Loading pretrained files for: embedding_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ECAPA_TDNN(\n",
       "  (blocks): ModuleList(\n",
       "    (0): TDNNBlock(\n",
       "      (conv): Conv1d(\n",
       "        (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (norm): BatchNorm1d(\n",
       "        (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): SERes2NetBlock(\n",
       "      (tdnn1): TDNNBlock(\n",
       "        (conv): Conv1d(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (activation): ReLU()\n",
       "        (norm): BatchNorm1d(\n",
       "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (res2net_block): Res2NetBlock_2(\n",
       "        (layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_2): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=same, dilation=(2,))\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (channel_layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (channel_layer_2_beta_1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (channel_layer_3_beta_2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (point_layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (point_layer_2_alpha_1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "        (point_layer_3_alpha_2): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (tdnn2): TDNNBlock(\n",
       "        (conv): Conv1d(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (activation): ReLU()\n",
       "        (norm): BatchNorm1d(\n",
       "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (se_block): SEBlock(\n",
       "        (conv1): Conv1d(\n",
       "          (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv1d(\n",
       "          (conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (2): SERes2NetBlock(\n",
       "      (tdnn1): TDNNBlock(\n",
       "        (conv): Conv1d(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (activation): ReLU()\n",
       "        (norm): BatchNorm1d(\n",
       "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (res2net_block): Res2NetBlock_2(\n",
       "        (layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_2): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=same, dilation=(3,))\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (channel_layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (channel_layer_2_beta_1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (channel_layer_3_beta_2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (point_layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (point_layer_2_alpha_1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "        (point_layer_3_alpha_2): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (tdnn2): TDNNBlock(\n",
       "        (conv): Conv1d(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (activation): ReLU()\n",
       "        (norm): BatchNorm1d(\n",
       "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (se_block): SEBlock(\n",
       "        (conv1): Conv1d(\n",
       "          (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv1d(\n",
       "          (conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (3): SERes2NetBlock(\n",
       "      (tdnn1): TDNNBlock(\n",
       "        (conv): Conv1d(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (activation): ReLU()\n",
       "        (norm): BatchNorm1d(\n",
       "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (res2net_block): Res2NetBlock_2(\n",
       "        (layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer_2): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=same, dilation=(4,))\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (channel_layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(2048, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (channel_layer_2_beta_1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (channel_layer_3_beta_2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "        (point_layer_1): TDNNBlock(\n",
       "          (conv): Conv1d(\n",
       "            (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,), padding=same)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (norm): BatchNorm1d(\n",
       "            (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (point_layer_2_alpha_1): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "        (point_layer_3_alpha_2): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
       "      )\n",
       "      (tdnn2): TDNNBlock(\n",
       "        (conv): Conv1d(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (activation): ReLU()\n",
       "        (norm): BatchNorm1d(\n",
       "          (norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (se_block): SEBlock(\n",
       "        (conv1): Conv1d(\n",
       "          (conv): Conv1d(512, 128, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv1d(\n",
       "          (conv): Conv1d(128, 512, kernel_size=(1,), stride=(1,), padding=same)\n",
       "        )\n",
       "        (sigmoid): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mfa): TDNNBlock(\n",
       "    (conv): Conv1d(\n",
       "      (conv): Conv1d(1536, 1536, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    )\n",
       "    (activation): ReLU()\n",
       "    (norm): BatchNorm1d(\n",
       "      (norm): BatchNorm1d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (asp): AttentiveStatisticsPooling(\n",
       "    (tdnn): TDNNBlock(\n",
       "      (conv): Conv1d(\n",
       "        (conv): Conv1d(4608, 128, kernel_size=(1,), stride=(1,), padding=same)\n",
       "      )\n",
       "      (activation): ReLU()\n",
       "      (norm): BatchNorm1d(\n",
       "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (tanh): Tanh()\n",
       "    (conv): Conv1d(\n",
       "      (conv): Conv1d(128, 1536, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    )\n",
       "  )\n",
       "  (asp_bn): BatchNorm1d(\n",
       "    (norm): BatchNorm1d(3072, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc): Conv1d(\n",
       "    (conv): Conv1d(3072, 192, kernel_size=(1,), stride=(1,), padding=same)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_on_main(params[\"pretrainer\"].collect_files)\n",
    "params[\"pretrainer\"].load_collected() ## 加载权重\n",
    "\n",
    "params[\"embedding_model\"].eval()\n",
    "params[\"embedding_model\"].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4715/4715 [00:52<00:00, 89.97it/s] \n",
      "100%|██████████| 4713/4713 [00:48<00:00, 97.99it/s] \n"
     ]
    }
   ],
   "source": [
    "# 第一次提取embedding ? \n",
    "enrol_dict_1 = compute_embedding_loop(enrol_dataloader,params)\n",
    "test_dict_1 = compute_embedding_loop(test_dataloader,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4715/4715 [00:48<00:00, 97.48it/s] \n",
      "100%|██████████| 4713/4713 [00:47<00:00, 98.27it/s] \n"
     ]
    }
   ],
   "source": [
    "# Second run (normalization stats are more stable)\n",
    "enrol_dict = compute_embedding_loop(enrol_dataloader,params)\n",
    "test_dict = compute_embedding_loop(test_dataloader,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [06:43<00:00, 99.22it/s] \n"
     ]
    }
   ],
   "source": [
    "if \"score_norm\" in params:                                                                                                          \n",
    "    train_dict = compute_embedding_loop(train_dataloader,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37611\n"
     ]
    }
   ],
   "source": [
    "with open(veri_file_path) as f:\n",
    "    veri_test = [line.rstrip() for line in f]\n",
    "print(len(veri_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_verification_scores(veri_test,params):\n",
    "    \"\"\" Computes positive and negative scores given the verification split.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    positive_scores = []\n",
    "    negative_scores = []\n",
    "\n",
    "    save_file = os.path.join(params[\"output_folder\"], \"scores.txt\")\n",
    "    s_file = open(save_file, \"w\")\n",
    "\n",
    "    # Cosine similarity initialization\n",
    "    similarity = torch.nn.CosineSimilarity(dim=-1, eps=1e-6)\n",
    "\n",
    "    # creating cohort for score normalization\n",
    "    if \"score_norm\" in params:\n",
    "        train_cohort = torch.stack(list(train_dict.values()))\n",
    "\n",
    "    for i, line in enumerate(veri_test):\n",
    "\n",
    "        # Reading verification file (enrol_file test_file label)\n",
    "        lab_pair = int(line.split(\" \")[0].rstrip().split(\".\")[0].strip())\n",
    "        enrol_id = line.split(\" \")[1].rstrip().split(\".\")[0].strip()\n",
    "        test_id = line.split(\" \")[2].rstrip().split(\".\")[0].strip()\n",
    "        enrol = enrol_dict[enrol_id]\n",
    "        test = test_dict[test_id]\n",
    "\n",
    "        if \"score_norm\" in params:\n",
    "            # Getting norm stats for enrol impostors\n",
    "            enrol_rep = enrol.repeat(train_cohort.shape[0], 1, 1)\n",
    "            score_e_c = similarity(enrol_rep, train_cohort)\n",
    "\n",
    "            if \"cohort_size\" in params:\n",
    "                score_e_c = torch.topk(\n",
    "                    score_e_c, k=params[\"cohort_size\"], dim=0\n",
    "                )[0]\n",
    "\n",
    "            mean_e_c = torch.mean(score_e_c, dim=0)\n",
    "            std_e_c = torch.std(score_e_c, dim=0)\n",
    "\n",
    "            # Getting norm stats for test impostors\n",
    "            test_rep = test.repeat(train_cohort.shape[0], 1, 1)\n",
    "            score_t_c = similarity(test_rep, train_cohort)\n",
    "\n",
    "            if \"cohort_size\" in params:\n",
    "                score_t_c = torch.topk(\n",
    "                    score_t_c, k=params[\"cohort_size\"], dim=0\n",
    "                )[0]\n",
    "\n",
    "            mean_t_c = torch.mean(score_t_c, dim=0)\n",
    "            std_t_c = torch.std(score_t_c, dim=0)\n",
    "\n",
    "        # Compute the score for the given sentence\n",
    "        score = similarity(enrol, test)[0]\n",
    "\n",
    "        # Perform score normalization\n",
    "        if \"score_norm\" in params:\n",
    "            if params[\"score_norm\"] == \"z-norm\":\n",
    "                score = (score - mean_e_c) / std_e_c\n",
    "            elif params[\"score_norm\"] == \"t-norm\":\n",
    "                score = (score - mean_t_c) / std_t_c\n",
    "            elif params[\"score_norm\"] == \"s-norm\":\n",
    "                score_e = (score - mean_e_c) / std_e_c\n",
    "                score_t = (score - mean_t_c) / std_t_c\n",
    "                score = 0.5 * (score_e + score_t)\n",
    "\n",
    "        # write score file\n",
    "        s_file.write(\"%s %s %i %f\\n\" % (enrol_id, test_id, lab_pair, score))\n",
    "        scores.append(score)\n",
    "\n",
    "        if lab_pair == 1:\n",
    "            positive_scores.append(score)\n",
    "        else:\n",
    "            negative_scores.append(score)\n",
    "\n",
    "    s_file.close()\n",
    "    return positive_scores, negative_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 生成分数\n",
    "positive_scores, negative_scores = get_verification_scores(veri_test,params)\n",
    "# del enrol_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## 给定一些类数据，计算等错误率，\n",
    "def compute_eer(fnr, fpr):\n",
    "    \"\"\" computes the equal error rate (EER) given FNR and FPR values calculated\n",
    "        for a range of operating points on the DET curve\n",
    "    \"\"\"\n",
    "\n",
    "    diff_pm_fa = fnr - fpr\n",
    "    x1 = np.flatnonzero(diff_pm_fa >= 0)[0]\n",
    "    x2 = np.flatnonzero(diff_pm_fa < 0)[-1]\n",
    "    a = (fnr[x1] - fpr[x1]) / (fpr[x2] - fpr[x1] - (fnr[x2] - fnr[x1]))\n",
    "    return fnr[x1] + a * (fnr[x2] - fnr[x1])\n",
    "\n",
    "\n",
    "def compute_pmiss_pfa(scores, labels):\n",
    "    \"\"\" computes false positive rate (FPR) and false negative rate (FNR)\n",
    "    given trial scores and their labels. A weights option is also provided\n",
    "    to equalize the counts over score partitions (if there is such\n",
    "    partitioning).\n",
    "    \"\"\"\n",
    "\n",
    "    sorted_ndx = np.argsort(scores)\n",
    "    labels = labels[sorted_ndx]\n",
    "\n",
    "    tgt = (labels == 1).astype('f8')\n",
    "    imp = (labels == 0).astype('f8')\n",
    "\n",
    "    fnr = np.cumsum(tgt) / np.sum(tgt)\n",
    "    fpr = 1 - np.cumsum(imp) / np.sum(imp)\n",
    "    return fnr, fpr\n",
    "\n",
    "\n",
    "def compute_min_cost(scores, labels, p_target=0.01):\n",
    "    fnr, fpr = compute_pmiss_pfa(scores, labels)\n",
    "    eer = compute_eer(fnr, fpr)\n",
    "    min_c = compute_c_norm(fnr, fpr, p_target)\n",
    "    return eer, min_c\n",
    "\n",
    "\n",
    "def compute_c_norm(fnr, fpr, p_target, c_miss=1, c_fa=1):\n",
    "    \"\"\" computes normalized minimum detection cost function (DCF) given\n",
    "        the costs for false accepts and false rejects as well as a priori\n",
    "        probability for target speakers\n",
    "    \"\"\"\n",
    "    dcf = c_miss * fnr * p_target + c_fa * fpr * (1 - p_target)\n",
    "    c_det = np.min(dcf)\n",
    "    c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n",
    "    return c_det/c_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37611 37611\n"
     ]
    }
   ],
   "source": [
    "scores_list = []\n",
    "label_list = []\n",
    "\n",
    "for i in range(len(positive_scores)):\n",
    "    scores_list.append(positive_scores[i].cpu().item())\n",
    "    label_list.append(1)\n",
    "\n",
    "\n",
    "for i in range(len(negative_scores)):\n",
    "    scores_list.append(negative_scores[i].cpu().item())\n",
    "    label_list.append(0)\n",
    "    \n",
    "print(len(scores_list),len(label_list))\n",
    "scores_list_n = np.array(scores_list)\n",
    "label_list_n = np.array(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871923015577647 0.11502723601489293 0.1808850122327412\n"
     ]
    }
   ],
   "source": [
    "eer, min_c = compute_min_cost(scores_list_n, label_list_n, p_target=0.01)\n",
    "\n",
    "_, min_c2 = compute_min_cost(scores_list_n, label_list_n, p_target=0.001)\n",
    "\n",
    "print(eer * 100,min_c,min_c2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
